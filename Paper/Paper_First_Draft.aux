\relax 
\citation{b13}
\citation{b46}
\citation{b14}
\citation{b17}
\citation{b18}
\citation{b19}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-A}}Image and Video Object Detection in general}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {I-A}1}History of image object detection}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {I-A}2}Types of image object detectors}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {I-A}3}Why is video object detection harder?}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-B}}Recurrent Neural Networks in general}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {I-B}1}Delimitation to non-recurrent Neural Networks}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {I-B}2}Common Types of Recurent Neural Networks}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Feature-based Video Object Detection}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Recurrent Multi-frame Single Shot Detector for Video Object Detection [1]}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture: Recurent Multi-frame single shot detector [1]. The Feature Extractor generates feature maps for four frames and feeds those feature maps into the GRUs. Detection Head uses the final feature maps which are created by the GRU with respect to the temporal context to create Bounding Boxes and Class probabilities\relax }}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Mobile Video Object Detection with Temporally Aware Feature Maps [2]}{2}}
\citation{b6}
\citation{b30}
\citation{b29}
\citation{b21}
\citation{b31}
\citation{b15}
\citation{b32}
\citation{b33}
\citation{b34}
\citation{b45}
\citation{b40}
\citation{b18}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Architecture Mobile Video Object Detection with temporally aware feature maps [2]. First 13 Convolutional Layeer create Feature Maps. Those Feature Maps are fed into the Bottleneck-LSTMs (faster LSTMs). Afterwards Convolutional-Layer create new Feature Maps and feed them again into Bottleneck-LSTMs.\relax }}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Feature Selective Small Object Detection via Knowledge-based recurrent attentive neural networks \cite  {b6}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Architecture of the model proposed in Feature Selective Small Object Detection via Knowledge-based recurrent attentive neural networks\relax }}{3}}
\citation{b7}
\citation{b35}
\citation{b35}
\citation{b31}
\citation{b36}
\citation{b18}
\citation{b17}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-D}}Looking fast and slow: memory-guided mobile video object detection \cite  {b7}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The KB-RANN architecture. The frames are passed through the feature extractors to obtain, feature maps. The blue MobileNetV2 feature extractors are the light-weight ones while the red one is computationally more expensive.\relax }}{4}}
\citation{b37}
\citation{b8}
\citation{b19}
\citation{b35}
\citation{b19}
\citation{b38}
\citation{b39}
\citation{b40}
\citation{b41}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The interleaved policy, is based on the LSTM states\relax }}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-E}}Detect to Track and track to detect\cite  {b8}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Architecture of the model proposed in Detect to Track and track to detect\relax }}{5}}
\citation{b42}
\citation{b43}
\citation{b44}
\citation{b10}
\@writefile{toc}{\contentsline {section}{\numberline {III}Box-Level-based Video Object Detection}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Box-Level-based Video Object Detection\relax }}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Optimizing Video Object Detection via a Scale-Time Lattice\cite  {b10}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The Scale-Time Lattice has an efficient design for reaching a balance between performance and accuracy\relax }}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The Propagation and Refinement Units (PRUs) are the constituent elements which the Scale-Time Lattice is made up of\relax }}{6}}
\citation{b5}
\citation{27}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Context Matters: Refining Object Detection in Video with Recurrent Neural Networks [3]}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Architecture Context Matters: Refining Object Detection in Video with Recurrent Neural Networks [4]. First the "Pseudolabler" creates bounding boxes and class probabilities for every input frame. Afterwards the GRU fuses the output of the current and some past frames and refines the bounding boxes and class probabilities.\relax }}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking [5]}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Architecture ROLO [5]. The architecture feeds feature representations made by the YOLO network as well as bounding boxes also made by the YOLO Network into a recurrent unit. The recurrent unit uses the temporal context of those inputs to finally output bounding boxes and class probabilities.\relax }}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Flow-based Object Detection}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Definition}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Deep Feature Flow for Video Recognition}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Architecture Deep Feature Flow for Video Recognition\relax }}{8}}
\citation{b1}
\citation{b6}
\citation{b8}
\citation{b8}
\citation{b10}
\citation{b10}
\citation{b3}
\citation{b3}
\citation{b7}
\citation{b2}
\citation{b6}
\citation{b4}
\citation{b5}
\@writefile{toc}{\contentsline {section}{\numberline {V}Comparison of different approaches}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}KITTI Dataset}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Results on KITTI Dataset\relax }}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}ImageNet Dataset}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Results on ImageNet Dataset\relax }}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Results on COCO Dataset}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Results on COCO Dataset\relax }}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-D}}Results on YouTube Dataset}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Results on YT Dataset\relax }}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-E}}Results on OTB Challenge Dataset}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Results on OTB Challenge Dataset\relax }}{9}}
\bibcite{b1}{1}
\bibcite{b2}{2}
\bibcite{b3}{3}
\bibcite{b4}{4}
\bibcite{b5}{5}
\bibcite{b6}{6}
\bibcite{b7}{7}
\bibcite{b8}{8}
\bibcite{b9}{9}
\bibcite{b10}{10}
\bibcite{b11}{11}
\bibcite{b12}{12}
\bibcite{b13}{13}
\bibcite{b14}{14}
\bibcite{b15}{15}
\bibcite{b16}{16}
\bibcite{b17}{17}
\bibcite{b18}{18}
\bibcite{b19}{19}
\bibcite{b20}{20}
\bibcite{b21}{21}
\bibcite{b22}{22}
\bibcite{b23}{23}
\bibcite{b24}{24}
\bibcite{b25}{25}
\bibcite{b26}{26}
\bibcite{b27}{27}
\bibcite{b28}{28}
\bibcite{b29}{29}
\bibcite{b30}{30}
\bibcite{b31}{31}
\bibcite{b32}{32}
\bibcite{b33}{33}
\bibcite{b34}{34}
\bibcite{b35}{35}
\bibcite{b36}{36}
\bibcite{b37}{37}
\bibcite{b38}{38}
\bibcite{b39}{39}
\bibcite{b40}{40}
\bibcite{b41}{41}
\bibcite{b42}{42}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Outro}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-A}}Conclusion}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-B}}Further work}{10}}
\@writefile{toc}{\contentsline {section}{References}{10}}
\bibcite{b43}{43}
\bibcite{b44}{44}
\bibcite{b45}{45}
\bibcite{b46}{46}
